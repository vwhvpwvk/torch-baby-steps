{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_babysteps as bstep\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from torch import nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8092\n",
    "N_EPOCHS = 20\n",
    "LR = 1e-4\n",
    "WD = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 60000 instances\n",
      "Validation set has 10000 instances\n"
     ]
    }
   ],
   "source": [
    "# load data \n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "data_transforms = {\n",
    "\n",
    "    'train': transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    # add random rotation\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize( mean = [0.485, 0.456, 0.406], \n",
    "                         std = [0.229, 0.224, 0.225])\n",
    "\n",
    "]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize( mean = [0.485, 0.456, 0.406], \n",
    "                         std = [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "\n",
    "}\n",
    "\n",
    "# Create datasets for training & validation, download if necessary\n",
    "training_set = datasets.FashionMNIST('./data', transform=transform, train=True, download=True) #FashionMNIST('./data', train=True, transform=transform, download=True)\n",
    "validation_set = datasets.FashionMNIST('./data', train=False, transform=transform, download=True) #FashionMNIST(\n",
    "\n",
    "# Create data loaders for our datasets; shuffle for training, not for validation\n",
    "training_loader = torch.utils.data.DataLoader(training_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Class labels\n",
    "classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "        'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')\n",
    "\n",
    "# Report split sizes\n",
    "print('Training set has {} instances'.format(len(training_set)))\n",
    "print('Validation set has {} instances'.format(len(validation_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. Using GPU: NVIDIA RTX 2000 Ada Generation Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# Optimizers specified in the torch.optim packagemodel = tbstep.FashionMNISTClassifier()\n",
    "model = bstep.FashionMNISTClassifier()\n",
    "training_set = datasets.FashionMNIST('./data', train=True, transform=transform, download=True)\n",
    "validation_set = datasets.FashionMNIST('./data', train=False, transform=transform, download=True)\n",
    "training_loader = torch.utils.data.DataLoader(training_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay = WD)\n",
    "device = bstep.get_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:09<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Epoch 1 Finished ---------- Average Loss: 2.3034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:08<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Epoch 2 Finished ---------- Average Loss: 2.2971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:08<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Epoch 3 Finished ---------- Average Loss: 2.2897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:08<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Epoch 4 Finished ---------- Average Loss: 2.2806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:08<00:00,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Epoch 5 Finished ---------- Average Loss: 2.2683\n",
      "----Training Finished-----\n",
      "Total Training Time: 43.65 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>loss</th>\n",
       "      <th>training_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2.303388</td>\n",
       "      <td>9.098040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2.297110</td>\n",
       "      <td>9.000985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2.289659</td>\n",
       "      <td>8.135369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2.280595</td>\n",
       "      <td>8.464772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2.268314</td>\n",
       "      <td>8.945993</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epoch      loss  training_time\n",
       "0      0  2.303388       9.098040\n",
       "1      1  2.297110       9.000985\n",
       "2      2  2.289659       8.135369\n",
       "3      3  2.280595       8.464772\n",
       "4      4  2.268314       8.945993"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bstep.train_classifier(model = model, \n",
    "            train_ds = training_loader,\n",
    "            loss_function =  loss_function, \n",
    "            optimizer = optimizer, \n",
    "            device = device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object.__init__() takes exactly one argument (the instance to initialize)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 27\u001b[0m\n\u001b[0;32m     23\u001b[0m warmup_steps \u001b[38;5;241m=\u001b[39m total_steps \u001b[38;5;241m*\u001b[39m warmup_epochs\u001b[38;5;241m/\u001b[39mtotal_epochs \u001b[38;5;66;03m#epochs = \u001b[39;00m\n\u001b[0;32m     24\u001b[0m decay_steps \u001b[38;5;241m=\u001b[39m total_steps \u001b[38;5;241m-\u001b[39m warmup_steps\n\u001b[1;32m---> 27\u001b[0m lr_scheduler \u001b[38;5;241m=\u001b[39m \u001b[43mWarmupCosineAnnealingLR\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdamW\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_lr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecay_steps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdecay_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfinal_lr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5e-4\u001b[39;49m\n\u001b[0;32m     33\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[33], line 7\u001b[0m, in \u001b[0;36mWarmupCosineAnnealingLR.__init__\u001b[1;34m(self, optimizer, warmup_steps, max_lr, decay_steps, final_lr, last_epoch)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecay_steps \u001b[38;5;241m=\u001b[39m decay_steps\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_lr \u001b[38;5;241m=\u001b[39m final_lr\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlast_epoch\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: object.__init__() takes exactly one argument (the instance to initialize)"
     ]
    }
   ],
   "source": [
    "class WarmupCosineAnnealingLR():\n",
    "    def __init__(self, optimizer, warmup_steps, max_lr, decay_steps, final_lr = 0, last_epoch = -1):\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.max_lr = max_lr\n",
    "        self.decay_steps = decay_steps\n",
    "        self.final_lr = final_lr\n",
    "        super().__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if self.last_epoch < self.warmup_steps: \n",
    "            return [self.initial_lr + (self.max_lr - self.initial_lr)*(self.last_epoch +1)/ self.warmup_steps for base_lr in self.base_lrs]\n",
    "        \n",
    "        else:\n",
    "            progress = (self.last_epoch - self.warmup_steps)\n",
    "            cosine_val = 0.5* (1 + torch.cos( torch.pi*  progress))\n",
    "            return [self.final_lr + (self.max_lr - self.final_lr)* cosine_val for base_lr in self.base_lrs]\n",
    "        \n",
    "total_steps = 100\n",
    "warmup_epochs = 5\n",
    "plateau_epochs = 10\n",
    "total_epochs = 30\n",
    "decay_epochs = total_steps - warmup_epochs - plateau_epochs\n",
    "warmup_steps = total_steps * warmup_epochs/total_epochs #epochs = \n",
    "decay_steps = total_steps - warmup_steps\n",
    "\n",
    "\n",
    "lr_scheduler = WarmupCosineAnnealingLR(\n",
    "    torch.optim.AdamW(model.parameters(), lr= 1e-4, weight_decay = 1e-5),\n",
    "    warmup_steps = warmup_steps, \n",
    "    max_lr = 1e-1,\n",
    "    decay_steps = decay_steps,\n",
    "    final_lr = 5e-4\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-2.6.0-py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
